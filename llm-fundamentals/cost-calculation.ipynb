{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38054cff-c78b-4e49-b85d-1f777981911d",
   "metadata": {},
   "source": [
    "## How Cost is Calculated\n",
    "\n",
    "Welcome! This notebook provides the practical demonstration for our lecture on cost calculation. We've discussed the theory: API usage is priced on a **pay-per-token** basis, and the cost is a function of both the **input tokens** you send and the **output tokens** you receive.\n",
    "\n",
    "The formula is:\n",
    "`Total Cost = (Input Tokens × Price per Input Token) + (Output Tokens × Price per Output Token)`\n",
    "\n",
    "Now, let's see how to apply this in practice. The most accurate way to calculate the cost of a specific API call is to use the `usage` object that is returned in the API's response. This object gives you the precise token counts, taking all guesswork out of the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b029c5-5411-4038-ab3a-081cded4bf53",
   "metadata": {},
   "source": [
    "## Defining the Pricing Structure\n",
    "\n",
    "First, let's define the pricing for the model we're using, `gpt-4o-mini`. It's a best practice to define these values as constants in your application so you can easily update them if prices change.\n",
    "\n",
    "> Note: The following prices are for gpt-4o-mini as of August 2025. Always check the official OpenAI pricing page for the most current rates. Prices are typically listed per 1 million tokens.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ae234-2899-466f-b92b-3d175199940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "from textwrap import dedent\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries and environment variables loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b52e292-55e9-4ff5-898c-2cae2725f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PRICING = {\n",
    "    \"gpt-4o-mini\": {\n",
    "        \"input_per_1m\": 0.15,\n",
    "        \"output_per_1m\": 0.60\n",
    "    }\n",
    "}\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-4o-mini\"\n",
    "\n",
    "price_info = MODEL_PRICING[MODEL_NAME.split(\"/\")[-1]]\n",
    "PRICE_PER_INPUT_TOKEN = price_info[\"input_per_1m\"] / 1_000_000\n",
    "PRICE_PER_OUTPUT_TOKEN = price_info[\"output_per_1m\"] / 1_000_000\n",
    "\n",
    "print(f\"Pricing for model: {MODEL_NAME}\")\n",
    "print(f\"Price per input token:\\t${PRICE_PER_INPUT_TOKEN:.10f}\")\n",
    "print(f\"Price per output token:\\t${PRICE_PER_OUTPUT_TOKEN:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1651bd2a-b878-4377-a72e-915530750f57",
   "metadata": {},
   "source": [
    "## Calculating Cost from API Response\n",
    "\n",
    "Now, we'll make a simple, low-cost API call. Our calculation logic will pull the correct prices from the dictionary we just defined. We'll then inspect the `usage` attribute from the API response to get the exact token counts for our transaction.\n",
    "\n",
    "This is the ideal method for logging, auditing, and managing costs in a production application, as it's both accurate and scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce233fbc-d91e-49a6-86e9-ecf4c78ae791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model, max_tokens=20):\n",
    "    print(\"--- Getting completion from LiteLLM ---\")\n",
    "\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful travel assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "def analyze_cost(response):\n",
    "    if not response:\n",
    "        print(\"Cannot analyze cost of a failed API call!\")\n",
    "        return\n",
    "\n",
    "    raw_model_name = response.model\n",
    "\n",
    "    if \"/\" in raw_model_name:\n",
    "        raw_model_name = raw_model_name.split(\"/\")[-1]\n",
    "\n",
    "    model_used = None\n",
    "    \n",
    "    for model in MODEL_PRICING.keys():\n",
    "        if raw_model_name.startswith(model):\n",
    "            model_used = model\n",
    "            break\n",
    "\n",
    "    price_info = MODEL_PRICING[model_used]\n",
    "\n",
    "    if not price_info:\n",
    "        print(f\"WARNING: Pricing for model '{model_used}' not found!\")\n",
    "        return\n",
    "\n",
    "    usage_data = response.usage\n",
    "    input_tokens = usage_data.prompt_tokens\n",
    "    output_tokens = usage_data.completion_tokens\n",
    "\n",
    "    price_per_input = price_info[\"input_per_1m\"] / 1_000_000\n",
    "    price_per_output = price_info[\"output_per_1m\"] / 1_000_000\n",
    "\n",
    "    input_cost = input_tokens * price_per_input\n",
    "    output_cost = output_tokens * price_per_output\n",
    "    total_cost = input_cost + output_cost\n",
    "\n",
    "    print(f\"\\n--- Cost breakdown for model: {model_used} ---\")\n",
    "    print(f\"Model response: {response.choices[0].message.content}\")\n",
    "    print(f\"Input tokens:\\t{input_tokens}\\t| Cost: ${input_cost:.8f}\")\n",
    "    print(f\"Output tokens:\\t{output_tokens}\\t| Cost: ${output_cost:.8f}\")\n",
    "    print(f\"Total cost of API call: ${total_cost:.8f}\")\n",
    "\n",
    "    return total_cost, input_cost, output_cost\n",
    "\n",
    "user_prompt = \"What is the capital of France, and what is it famous for?\"\n",
    "\n",
    "response = get_completion(\n",
    "    user_prompt,\n",
    "    model=MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca3d2e1-55bb-4965-bed1-54b4ca5e84e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_cost(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d38200c",
   "metadata": {},
   "source": [
    "## Conversation Cost Analysis\n",
    "\n",
    "Now let's explore how costs accumulate during a multi-turn conversation. Each API call includes the entire conversation history, which means the input token count (and therefore cost) increases with each exchange.\n",
    "\n",
    "We'll simulate a 4-round conversation to demonstrate this cost escalation pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_conversation():\n",
    "    conversation_history = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": dedent(\"\"\"\n",
    "            You are a senior software engineer well versed in Python.\n",
    "            Your answers are comprehensive and intuitive to follow.\n",
    "            \"\"\")\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    conversation_rounds = [\n",
    "        \"What is Python and what makes it popular for development?\",\n",
    "        \"How do I create a virtual environment in that language?\", \n",
    "        \"What are the top 3 most important libraries for beginners?\",\n",
    "        \"Can you show me a simple example of using one of those libraries?\"\n",
    "    ]\n",
    "\n",
    "    total_cumulative_cost = 0\n",
    "\n",
    "    for round_num, user_message in enumerate(conversation_rounds, start=1):\n",
    "        print(f\"--- Round {round_num} ---\")\n",
    "        print(f\"User message: {user_message}\")\n",
    "\n",
    "        conversation_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message\n",
    "        })\n",
    "\n",
    "        response = litellm.completion(\n",
    "            model=MODEL_NAME,\n",
    "            messages=conversation_history\n",
    "        )\n",
    "\n",
    "        assistant_message = response.choices[0].message.content\n",
    "\n",
    "        conversation_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": assistant_message\n",
    "        })\n",
    "\n",
    "        print()\n",
    "        print(\"+\" * 50)\n",
    "        total_cost = analyze_cost(response)[0]\n",
    "        total_cumulative_cost += total_cost\n",
    "        print(\"+\" * 50)\n",
    "        print()\n",
    "        \n",
    "    print(\"\\n=== SUMMARY ===\")\n",
    "    print(f\"Total conversation cost: ${total_cumulative_cost:.8f}\")\n",
    "    print(f\"Average cost per round: ${total_cumulative_cost/len(conversation_rounds):.8f}\")\n",
    "    print(f\"Final conversation length: {len(conversation_history)} messages\")\n",
    "\n",
    "simulate_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e2f17-1444-4e4d-a9ba-a6acc7858fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
