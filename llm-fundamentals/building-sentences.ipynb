{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a11185f7-1146-438f-b6c1-20e54c177f8e",
   "metadata": {},
   "source": [
    "# How LLMs build sentences\n",
    "\n",
    "This notebook demonstrates the core mechanism behind how Large Language Models generate text. As we discussed in the lecture, an LLM generates its response **one token at a time** through a process called auto-regression. It repeatedly predicts the most likely next token based on all the tokens that came before it.\n",
    "\n",
    "This demonstration will make that process tangible. We will explore two key features of the OpenAI API that let us peek under the hood:\n",
    "\n",
    "1. **Streaming:** We'll see the tokens arrive one by one in real-time, just as the model produces them.\n",
    "2. **Log Probs (`logprobs`):** We'll actually look at the probability scores the model assigns to potential next tokens, proving it's a statistical process.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f8623-fc86-408b-9a2a-b7c99801124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "import math\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-4o-mini\"\n",
    "MAX_TOKENS = 200\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2c18da-fb27-42ef-b3ef-8d0c7647c945",
   "metadata": {},
   "source": [
    "## Watching Generation with Streaming\n",
    "\n",
    "The simplest way to observe the token-by-token process is to \"stream\" the response. Instead of sending a prompt and waiting for the entire completed text, streaming sends us each token as soon as it's generated. This is not only great for creating a responsive, real-time user experience (like in ChatGPT) but also perfectly illustrates the auto-regressive nature of LLMs.\n",
    "\n",
    "> Use Case: Use streaming in any application where you want to display the model's output to a user in real-time. It dramatically improves the user's perception of speed.\n",
    "> \n",
    "\n",
    "In the code below, we'll send a simple prompt and print each piece of the response as it comes in. Notice how the sentence is built up incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a30dc-f0c9-4e35-9859-a5b55904b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_story = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a short story about a typical day in a programmer's life.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = litellm.completion(\n",
    "    model=MODEL_NAME,\n",
    "    messages=prompt_story,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "print(\"--- Response ---\")\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915397e5-d1d4-4f83-8422-4acfdef33fa8",
   "metadata": {},
   "source": [
    "## Visualizing Probabilities with `logprobs`\n",
    "\n",
    "Now, let's get even closer to the model's \"decision\" process. We can ask the API to return the **log probabilities** for the most likely tokens at each step of the generation. This lets us see the statistical path the model is taking.\n",
    "\n",
    "A log probability is just a mathematical way to represent very small probability numbers. We can easily convert them back to percentages to see exactly what the model was \"thinking.\"\n",
    "\n",
    "> Note: Requesting logprobs can be very useful for debugging prompts, understanding model behavior, or implementing advanced techniques like constraining model output.\n",
    "> \n",
    "\n",
    "In this example, we'll ask the model to complete the phrase \"The capital of France is\" and return the top 5 most likely next tokens. You'll see which token it chose and what the other top contenders were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8077c24f-244d-4c58-9cda-3840f276ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_capital = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Today I'm feeling\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response_capital = litellm.completion(\n",
    "    model=MODEL_NAME,\n",
    "    messages=prompt_capital,\n",
    "    max_tokens=10,\n",
    "    logprobs=True,\n",
    "    top_logprobs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52963ba4-0d6a-4a89-b737-24703491a1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_logprobs = response_capital.choices[0].logprobs.content[0].top_logprobs\n",
    "chosen_token = response_capital.choices[0].message.content\n",
    "\n",
    "print(f\"Prompt: {prompt_capital[0]['content']}\")\n",
    "print(f\"The model chose the token: {chosen_token}\")\n",
    "print(\"Here are the top 5 most likely next tokens:\")\n",
    "\n",
    "for logprob in top_logprobs:\n",
    "    prob_percentage = math.exp(logprob.logprob) * 100\n",
    "    print(f\"  - Token: '{logprob.token}', Prob: {prob_percentage:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fef13b-135e-410e-9f02-400e17b71a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
