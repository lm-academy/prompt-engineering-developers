{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87169f34-178d-45a0-810b-ed0250f8c29c",
   "metadata": {},
   "source": [
    "# Local LLMs with LiteLLM and Ollama\n",
    "\n",
    "Hello everyone! In the last lecture, we saw how LiteLLM can unify APIs for different providers. Now, we're going to explore another powerful capability: running Large Language Models **locally on your own machine** using a tool called **Ollama**.\n",
    "\n",
    "Why run models locally?\n",
    "\n",
    "- **Privacy & Security:** Your data never leaves your computer.\n",
    "- **No API Keys:** You don't need to manage secret keys for local models.\n",
    "- **Cost-Effective:** There are no per-token costs.\n",
    "- **Offline Capability:** You can use your models even without an internet connection.\n",
    "\n",
    "This notebook will guide you through installing Ollama, downloading a local model, and interacting with it using the same simple LiteLLM interface we've already learned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb3aacc-a92d-4c08-b863-d01497f0e059",
   "metadata": {},
   "source": [
    "## Step 1: Installing and Setting Up Ollama\n",
    "\n",
    "LiteLLM acts as the client, but first, we need to set up the local \"server\" that will run the models. This is where Ollama comes in.\n",
    "\n",
    "### 1a. Install Ollama\n",
    "\n",
    "Ollama is an application that makes it incredibly easy to download and run open-source LLMs like Llama 3, Phi-3, and Mistral.\n",
    "\n",
    "- Go to https://ollama.com/ and download the application for your operating system (macOS, Windows, or Linux).\n",
    "- Follow the installation instructions. Once installed, Ollama will run as a background service on your machine.\n",
    "\n",
    "### 1b. Pull a Local Model from the Terminal\n",
    "\n",
    "With Ollama installed, you can now download models. We'll use Meta's new **Llama 3.2** model for our example.\n",
    "\n",
    "Open your terminal (or Command Prompt/PowerShell on Windows) and run the following command:\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2\n",
    "```\n",
    "\n",
    "This will download the model to your machine. The first time you do this, it might take a while depending on your internet connection and the model's size.\n",
    "\n",
    "> Note: You can find a full list of available models on the Ollama Library page. Please keep in mind that more advanced models require powerful hardware!\n",
    "\n",
    "Now that our local Ollama server is running with a model downloaded, we can connect to it from our Python code using LiteLLM.\n",
    "\n",
    "### 2a. Library Setup\n",
    "\n",
    "We just need the `litellm` library. Notice that we don't need `python-dotenv` for this task, as there are no API keys to manage!\n",
    "\n",
    "### 2b. Making the Call\n",
    "\n",
    "To call a model running on Ollama, we use the same `litellm.completion()` function we used before. The only difference is the `model` string, which follows the format `ollama/<model_name>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6304e774-a42c-4a1b-a8d7-3f561880b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "MODEL_NAME = \"ollama/llama3.2\"\n",
    "MAX_TOKENS_DEFAULT = 500\n",
    "\n",
    "print(\"LiteLLM library is ready to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9133b8f-010f-4921-b277-d525042be998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
