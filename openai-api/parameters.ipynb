{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82073e6d-5662-48a1-bd70-c69bbbef92e6",
   "metadata": {},
   "source": [
    "# Controlling the Output - Key Parameters\n",
    "\n",
    "Hello and welcome back. We know how to send a prompt and get a response, but to truly engineer our outputs, we need to learn how to control the generation process itself. This is done by using optional parameters in our API calls.\n",
    "\n",
    "These parameters are the control knobs for the AI. They let us fine-tune everything from creativity and length to format and stopping points. This notebook will provide a hands-on demonstration of the most important ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2fc87b-dde0-4c86-bb78-d4f94fbc2d19",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import the necessary libraries and define a few variables to avoid duplicating information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c5129-9956-4802-a49e-45d5ac1562de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import litellm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-4o-mini\"\n",
    "\n",
    "def get_completion(messages, **kwargs):\n",
    "    if \"model\" not in kwargs:\n",
    "        kwargs[\"model\"] = MODEL_NAME\n",
    "        \n",
    "    return litellm.completion(\n",
    "        messages=messages,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f498505e-e3b2-4e25-8163-14a3d725c677",
   "metadata": {},
   "source": [
    "## `temperature` - The Creativity Knob\n",
    "\n",
    "`temperature` controls the randomness of the output. A low temperature (for example, 0.1) is deterministic and good for factual tasks. A high temperature (for example, 1.5) is creative and good for brainstorming.\n",
    "\n",
    "Let's ask the model to brainstorm a name for a new Python testing library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56574a9-8186-46ce-977c-c8e0140d6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING_LIBRARY_NAME = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Brainstorm a creative name for a new Python testing library.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "first_completion = get_completion(\n",
    "    TESTING_LIBRARY_NAME, \n",
    "    temperature=0.1\n",
    ").choices[0].message.content\n",
    "\n",
    "second_completion = get_completion(\n",
    "    TESTING_LIBRARY_NAME, \n",
    "    temperature=1.5\n",
    ").choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0194087a-4099-4e10-b9f6-8e80c614c3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_completion)\n",
    "print(second_completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3c21d9-e669-4100-b04c-a56e8ed96468",
   "metadata": {},
   "source": [
    "## `max_tokens` - The Safety Brake\n",
    "\n",
    "`max_tokens` sets a hard limit on the number of tokens the model can generate. This is crucial for controlling costs and preventing overly long responses.\n",
    "\n",
    "Let's see what happens when we ask for a summary with a very restrictive limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d62ef-94a9-4962-9a9c-af4660eb7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_PROMPT = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Summarize the concept of OOP in two sentences.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "short_completion = get_completion(\n",
    "    SUMMARY_PROMPT,\n",
    "    max_tokens=10\n",
    ")\n",
    "\n",
    "long_completion = get_completion(\n",
    "    SUMMARY_PROMPT,\n",
    "    max_tokens=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ecb5f6-02ee-444a-8b01-3fcfe2cb1471",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- max_tokens: 10 (will be cut off)\")\n",
    "print(short_completion.choices[0].message.content)\n",
    "print(\"\\n--- max_tokens: 100 (will probably not cut off)\")\n",
    "print(long_completion.choices[0].message.content)\n",
    "print(f\"\\nTotal tokens: {long_completion.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb48448c-4bb5-4409-8361-a36759aacbfb",
   "metadata": {},
   "source": [
    "## `stop` - The Clean Ending\n",
    "\n",
    "The `stop` parameter tells the model to stop generating as soon as it encounters a specific sequence of characters. This is perfect for generating lists or other structured data where you want to prevent extra conversational text.\n",
    "\n",
    "Let's ask for a list of programming paradigms and stop after the first item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25bab7-e481-4a88-bb18-ab2747047577",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_PROMPT = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"List the top 3 programming paradigms. Start with '1.'.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "no_stop_parameter = get_completion(\n",
    "    LIST_PROMPT\n",
    ")\n",
    "\n",
    "with_stop_parameter = get_completion(\n",
    "    LIST_PROMPT,\n",
    "    stop=\"\\n3.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d4f8e0-c0d6-49b1-bda8-4adc6dd70670",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- without stop parameter---\")\n",
    "print(no_stop_parameter.choices[0].message.content)\n",
    "print(\"\\n--- with stop parameter---\")\n",
    "print(with_stop_parameter.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c1194-dfa8-4a7d-bce4-5117e90ca40a",
   "metadata": {},
   "source": [
    "## `n` - Generating Multiple Choices\n",
    "\n",
    "The `n` parameter lets you get multiple different responses for the same prompt in a single API call. This is great for brainstorming when combined with a higher temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d217b4-f70d-4739-8bb1-6e5f14ef7866",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLOGAN_PROMPT = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write a marketing slogan for my newly created AI-powered debug tool.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response_n_choices = get_completion(\n",
    "    SLOGAN_PROMPT,\n",
    "    temperature=1.5,\n",
    "    n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c34e98-9522-4ec1-9e9b-738c0010643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, choice in enumerate(response_n_choices.choices, start=1):\n",
    "    print(f\"Slogan {i}: {choice.message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776476dd-6332-4959-bd92-f20fcd0c55dd",
   "metadata": {},
   "source": [
    "## `response_format` - Guaranteed JSON\n",
    "\n",
    "For developers, this is a game-changer. By setting `response_format={\"type\": \"json_object\"}`, you can force the model to output a syntactically correct JSON object. This is only available on newer models like `gpt-4o-mini` and is incredibly reliable.\n",
    "\n",
    "Let's extract information from a sentence into a structured JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59031bf-5071-4d8d-a34b-43c7e0502b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_PROMPT = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Extract a JSON object with the user's name, city, and product they are asking about.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hi, this is Alex from Berlin. I have a question about the 'Quantum03' server.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "normal_completion = get_completion(JSON_PROMPT)\n",
    "json_completion = get_completion(\n",
    "    JSON_PROMPT, \n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bf2262-4915-4b96-9d96-a2db164d33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(json_str):\n",
    "    try:\n",
    "        parsed_json = json.loads(json_str)\n",
    "        print(\"\\nSuccessfully parsed JSON:\")\n",
    "        print(parsed_json)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"\\nFailed to parse JSON: {e}\")\n",
    "\n",
    "parse_json(normal_completion.choices[0].message.content)\n",
    "parse_json(json_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdd21cd-4d11-4472-a55b-f4b167053858",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normal_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd40502-fca2-4489-8218-5e3e44a57e23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
