{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a6f5eff-cfdf-433d-8591-43c39f0fadc7",
   "metadata": {},
   "source": [
    "# Unifying APIs with LiteLLM\n",
    "\n",
    "Hello everyone! In the real world, the LLM landscape is vast and constantly changing. New, powerful models are released all the time from providers like Google, Anthropic, Meta, and more.\n",
    "\n",
    "**This presents a major engineering challenge:** how do you use the best model for the job without rewriting your code every time? This is the problem that a brilliant open-source tool called **LiteLLM** solves.\n",
    "\n",
    "LiteLLM is a **unified translation layer**. It allows you to call over 100 different LLM providers using the exact same format you've already learned. You write your code once, and LiteLLM handles the complex work of translating your request to the specific format each provider needs.\n",
    "\n",
    "This notebook will demonstrate how to use LiteLLM to seamlessly switch between different AI providers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5688e30d-8ed1-4ee5-b67a-eafde3aef41e",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "\n",
    "First, we need to install the `litellm` library. We also need to remember our security rule: each new provider will require its own API key, which must be set as an environment variable.\n",
    "\n",
    "> Important: To run the final example in this notebook, you will need an API key from another provider, such as Anthropic (for Claude models). You would add this to your `.env` file just like you did for OpenAI.\n",
    "> \n",
    "> \n",
    "> **Example `.env` file:**\n",
    "> \n",
    "> ```\n",
    "> OPENAI_API_KEY=\"sk-...\"\n",
    "> ANTHROPIC_API_KEY=\"sk-ant-...\"\n",
    "> \n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "575180f1-90b2-4da7-9d49-1b4d077f2a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries installed and environment variables loaded.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import litellm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries installed and environment variables loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d80b8cd2-aa1b-4379-ad5d-1bc73fbfdb77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54764cea-607d-4c74-9f77-c7abc4be707f",
   "metadata": {},
   "source": [
    "## The Baseline - A Standard OpenAI Call\n",
    "\n",
    "Let's start with a standard API call using the `openai` library. This is our familiar baseline. We'll ask a simple question to see the standard response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c8098e-3302-49ba-a58a-4f0a18a7db2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2d6a7d4-1215-47a5-a863-3b0062ce6fd7",
   "metadata": {},
   "source": [
    "## LiteLLM Calling an OpenAI Model\n",
    "\n",
    "Now, let's make the exact same call, but this time using `litellm`. Notice two things:\n",
    "\n",
    "1. We call the `litellm.completion()` function.\n",
    "2. The `model` string is now prefixed with the provider: `\"openai/gpt-4o-mini\"`.\n",
    "\n",
    "The rest of the code structure is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30c17742-8dbe-4de1-830b-31fca0a8e3de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0457e2e0-f826-4d35-a10b-ff0058832edb",
   "metadata": {},
   "source": [
    "## The Real Power - Switching to Another Provider\n",
    "\n",
    "This is where LiteLLM's magic shines. We are now going to call a completely different model from a different company: Anthropic's Claude 3.5 Haiku.\n",
    "\n",
    "The **only** thing we change in our code is the `model` string. We don't change the function call, the parameters, or the way we parse the response. As long as our `ANTHROPIC_API_KEY` is set in our `.env` file, LiteLLM handles everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1a4ff0-6bdd-418c-9176-d48f156ded5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
