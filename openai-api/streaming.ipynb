{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58d0fd93-3a72-44a0-ae47-21c4536e9fc3",
   "metadata": {},
   "source": [
    "# Real-Time Interaction with Streaming\n",
    "\n",
    "Hello everyone. So far, our API calls have followed a standard pattern: we send a request, wait for the *entire* response to be generated, and then receive it all at once. This is fine for quick tasks, but for longer generations, it can lead to a poor user experience with a lot of waiting.\n",
    "\n",
    "This is where **streaming** comes in. Instead of waiting for the full response, streaming lets us receive the output **a few tokens at a time, as they're being generated.** This is how applications like ChatGPT feel so fast and responsive.\n",
    "\n",
    "Let's see how to implement this simple but powerful feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b4dce-a67d-4f39-b67a-fc1b2da037e9",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To appreciate the benefit of streaming, we need a task that isn't instantaneous. We'll ask the model to generate a detailed explanation of a programming concept. This will take a few seconds, making the difference between blocking and streaming very clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d80ac40-d9ae-4a66-9284-ea004a5a1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import litellm\n",
    "from textwrap import dedent\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-4o-mini\"\n",
    "MAX_TOKENS = 200\n",
    "\n",
    "long_prompt = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": dedent(\"\"\"\n",
    "            You are a senior Python developer tutoring a junior colleague.\n",
    "            Explain the difference between concurrency and parallelism in Python.\n",
    "            Keep it concise.\n",
    "        \"\"\")\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f72e22d-6d37-4baa-8962-60dedcf71a0c",
   "metadata": {},
   "source": [
    "## The Blocking Request\n",
    "\n",
    "First, let's make a standard API call. When you run the cell below, pay close attention to the pause between when you execute it and when the full text appears. This waiting period is what we want to eliminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b20a6d-6e7e-44d7-8ed5-0727dc526258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "659d2e9a-4c5d-4791-aff5-8343dc807b28",
   "metadata": {},
   "source": [
    "## The Streaming Request\n",
    "\n",
    "Now, let's make the exact same request, but this time we'll add one crucial parameter: `stream=True`.\n",
    "\n",
    "When we do this, the function no longer returns a complete response object. Instead, it returns a **generator**. We can then loop through this generator to get each \"chunk\" of the response as it's created. This allows us to print the text to the screen almost instantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91112c3f-cbfb-4e23-a731-fee353734879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69176796-6da9-4560-8a3b-4ce8eae5570c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
