{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58d0fd93-3a72-44a0-ae47-21c4536e9fc3",
   "metadata": {},
   "source": [
    "# Real-Time Interaction with Streaming\n",
    "\n",
    "Hello everyone. So far, our API calls have followed a standard pattern: we send a request, wait for the *entire* response to be generated, and then receive it all at once. This is fine for quick tasks, but for longer generations, it can lead to a poor user experience with a lot of waiting.\n",
    "\n",
    "This is where **streaming** comes in. Instead of waiting for the full response, streaming lets us receive the output **a few tokens at a time, as they're being generated.** This is how applications like ChatGPT feel so fast and responsive.\n",
    "\n",
    "Let's see how to implement this simple but powerful feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b4dce-a67d-4f39-b67a-fc1b2da037e9",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To appreciate the benefit of streaming, we need a task that isn't instantaneous. We'll ask the model to generate a detailed explanation of a programming concept. This will take a few seconds, making the difference between blocking and streaming very clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d80ac40-d9ae-4a66-9284-ea004a5a1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import litellm\n",
    "from textwrap import dedent\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-4o-mini\"\n",
    "MAX_TOKENS = 200\n",
    "\n",
    "long_prompt = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": dedent(\"\"\"\n",
    "            You are a senior Python developer tutoring a junior colleague.\n",
    "            Explain the difference between concurrency and parallelism in Python.\n",
    "            Keep it concise.\n",
    "        \"\"\")\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f72e22d-6d37-4baa-8962-60dedcf71a0c",
   "metadata": {},
   "source": [
    "## The Blocking Request\n",
    "\n",
    "First, let's make a standard API call. When you run the cell below, pay close attention to the pause between when you execute it and when the full text appears. This waiting period is what we want to eliminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6b20a6d-6e7e-44d7-8ed5-0727dc526258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Making a standard blocking call---\n",
      "Request completed in 5.5574 seconds.\n",
      "\n",
      "Full response:\n",
      "Certainly! \n",
      "\n",
      "**Concurrency** and **Parallelism** are both concepts related to executing multiple tasks, but they have distinct meanings:\n",
      "\n",
      "1. **Concurrency**:\n",
      "   - Refers to the ability of a program to manage multiple tasks at once. This doesnâ€™t necessarily mean they are executed simultaneously; rather, they may progress independently within the same time frame.\n",
      "   - In Python, concurrency is often achieved using frameworks like `asyncio`, which allows for handling asynchronous operations. The tasks share a single thread but can yield control while waiting for resources, making it seem like they are running at the same time.\n",
      "\n",
      "2. **Parallelism**:\n",
      "   - Involves executing multiple tasks simultaneously, leveraging multiple CPU cores. This means tasks are truly running at the same time, which can lead to significant performance enhancements for CPU-bound tasks.\n",
      "   - In Python, parallelism can be achieved using the `multiprocessing` module, which allows you to create multiple processes, each running on its own core.\n",
      "\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Making a standard blocking call---\")\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "response_blocking = litellm.completion(\n",
    "    model=MODEL_NAME,\n",
    "    messages=long_prompt,\n",
    "    max_tokens=MAX_TOKENS\n",
    ")\n",
    "\n",
    "content = response_blocking.choices[0].message.content\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "print(f\"Request completed in {end_time - start_time:.4f} seconds.\")\n",
    "print(\"\\nFull response:\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d2e9a-4c5d-4791-aff5-8343dc807b28",
   "metadata": {},
   "source": [
    "## The Streaming Request\n",
    "\n",
    "Now, let's make the exact same request, but this time we'll add one crucial parameter: `stream=True`.\n",
    "\n",
    "When we do this, the function no longer returns a complete response object. Instead, it returns a **generator**. We can then loop through this generator to get each \"chunk\" of the response as it's created. This allows us to print the text to the screen almost instantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91112c3f-cbfb-4e23-a731-fee353734879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Making a streaming call---\n",
      "Certainly! \n",
      "\n",
      "**Concurrency** and **parallelism** are two concepts often used in programming, particularly in Python, but they serve different purposes.\n",
      "\n",
      "- **Concurrency** is about dealing with many tasks at the same time but not necessarily executing them simultaneously. This means the system can handle multiple tasks by switching between them, making progress on each task as resources become available. In Python, this is often achieved using asynchronous programming (e.g., `asyncio`) or threading. However, due to Global Interpreter Lock (GIL), only one thread can execute Python bytecode at a time in CPython, which limits true parallel execution in threads.\n",
      "\n",
      "- **Parallelism**, on the other hand, involves executing multiple tasks at the exact same time, which is possible when you have multiple processors or cores. In Python, this can be achieved using the `multiprocessing` module, which creates separate processes that can run independently and truly execute in parallel, bypassing the limitations of the GIL.\n",
      "\n",
      "In summary:\n",
      "\n",
      "\n",
      "Request completed in 5.1098 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Making a streaming call---\")\n",
    "start_time = time.perf_counter()\n",
    "full_response_streaming = \"\"\n",
    "\n",
    "response_streaming = litellm.completion(\n",
    "    model=MODEL_NAME,\n",
    "    messages=long_prompt,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response_streaming:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        chunk_content = chunk.choices[0].delta.content\n",
    "        print(chunk_content, end=\"\", flush=True)\n",
    "        full_response_streaming += chunk_content\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "print(f\"\\n\\nRequest completed in {end_time - start_time:.4f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69176796-6da9-4560-8a3b-4ce8eae5570c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
