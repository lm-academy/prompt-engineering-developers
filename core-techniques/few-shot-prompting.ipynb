{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa16ca57-5ca1-428a-9ac5-f400da8ecadd",
   "metadata": {},
   "source": [
    "# Guiding with Few-Shot Examples\n",
    "\n",
    "Hello everyone. Today, we're exploring one of the most reliable techniques for getting structured, predictable output from an LLM. It's based on a classic rule in writing: **Show, don't just tell.** This is the core idea behind **Few-Shot Prompting**.\n",
    "\n",
    "A **zero-shot** prompt asks the model to perform a task by only *describing* it. A **few-shot** prompt, however, *shows* the model what to do by including a few high-quality examples of the task directly in the prompt. This is a form of \"in-context learning\" and is incredibly effective for tasks that require a specific output format.\n",
    "\n",
    "This notebook will demonstrate how to build a few-shot prompt to solve a common developer task: **converting natural language user queries into a structured JSON command.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd75d37-16c8-443b-a8a6-02ddcd00b8f7",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "First, let's set up our reusable helper functions for both `openai` and `litellm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80289627-1719-4a38-9319-0bb109b178ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from textwrap import dedent\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-4o-mini\"\n",
    "MAX_TOKENS_DEFAULT = 200\n",
    "\n",
    "def get_completion(\n",
    "    prompt,\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=MAX_TOKENS_DEFAULT,\n",
    "    **kwargs\n",
    "):\n",
    "    parsed_messages = []\n",
    "\n",
    "    if type(prompt) is str:\n",
    "        parsed_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        parsed_messages = prompt\n",
    "\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=parsed_messages,\n",
    "        max_tokens=max_tokens,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"Helper functions are defined and ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c9ac4e3-e2fe-409c-9ad8-7b5cf6f32dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Can you find my ticket #T-5829?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a845ef6-696b-4833-902d-e30399926c43",
   "metadata": {},
   "source": [
    "## The Zero-Shot Attempt (Telling the Model)\n",
    "\n",
    "Let's first try to solve this with a zero-shot prompt. We'll describe the JSON format we want in the system prompt and then provide a user query. This approach \"tells\" the model what to do, but doesn't show it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43351d8f-46e4-4c4e-b635-488539b59f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dbe829d-4d80-448e-8683-fe6eeef25ae4",
   "metadata": {},
   "source": [
    "## The Few-Shot Solution (Showing the Model)\n",
    "\n",
    "Now, let's build a **few-shot prompt**. We will provide a sequence of high-quality examples directly in the prompt. For chat models, the correct way to do this is by creating a \"conversation\" where we act as both the `user` (providing the input) and the `assistant` (providing the correct output).\n",
    "\n",
    "The model learns the pattern from this conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f3e8df-3627-4c5d-89b5-bfef226515e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_query = \"Can you find my ticket #T-5829?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddfb707-9a3f-4d35-b396-14a115c0de55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
