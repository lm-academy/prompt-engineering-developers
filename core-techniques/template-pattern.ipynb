{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b39bac4d-2e44-4910-ae56-a27add50a937",
   "metadata": {},
   "source": [
    "# The Template Pattern - Forcing a Structured Response\n",
    "\n",
    "Hello everyone. Whenever interacting with LLMs, it will very often be the case that the model's reasoning and its final answer are mixed together in a free-form block of text, making it difficult for our programs to use the output reliably.\n",
    "\n",
    "This is where a highly practical and robust technique comes in: **The Template Pattern.**\n",
    "\n",
    "The core idea is profound: instead of letting the model decide the structure of its response, **we force the model to fill in a predefined template.** This gives us complete control over the entire output, making it perfectly and predictably parsable.\n",
    "\n",
    "This notebook will demonstrate how to use this pattern to separate the model's \"thinking\" from its final \"answer.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fe9695-81f7-40c1-8a38-9a2333702cfb",
   "metadata": {},
   "source": [
    "## The Task and Helper Functions\n",
    "\n",
    "Our task will be to refactor a piece of Python code. We want the model to first *think* about the necessary changes and then provide the final code. We will compare a standard Chain-of-Thought response with a templated one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12fcdc-ac11-435a-aaef-a2ea320b72b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import litellm\n",
    "from IPython.display import display, Markdown\n",
    "from textwrap import dedent\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-4o-mini\"\n",
    "MAX_TOKENS_DEFAULT = 500\n",
    "\n",
    "# --- The \"messy\" code to be refactored ---\n",
    "def get_user_initials(user_data_list):\n",
    "    initials_list = []\n",
    "    for user in user_data_list:\n",
    "        if 'name' in user and len(user['name']) > 0:\n",
    "            parts = user['name'].split(' ')\n",
    "            initials = ''\n",
    "            for part in parts:\n",
    "                initials += part[0].upper()\n",
    "            initials_list.append(initials)\n",
    "    return initials_list\n",
    "\n",
    "CODE_TO_REFACTOR = inspect.getsource(get_user_initials)\n",
    "\n",
    "def get_completion(\n",
    "    prompt,\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=MAX_TOKENS_DEFAULT,\n",
    "    **kwargs\n",
    "):\n",
    "    parsed_messages = []\n",
    "\n",
    "    if type(prompt) is str:\n",
    "        parsed_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        parsed_messages = prompt\n",
    "\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=parsed_messages,\n",
    "        max_tokens=max_tokens,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"Setup complete. Helper functions and code context are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf72ae-9997-491c-ad7a-e08766cc1222",
   "metadata": {},
   "source": [
    "## Unstructured Chain-of-Thought\n",
    "\n",
    "Let's start by using a standard Chain-of-Thought prompt. We'll ask the model to think step-by-step and then provide the code. Watch how the reasoning and the final code are intertwined in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b5d047-a87d-4e89-8519-73d302e059c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc58069e-7534-4eaf-a8ea-28286c58c8cf",
   "metadata": {},
   "source": [
    "## The Template Pattern\n",
    "\n",
    "Now, we'll solve this problem by providing a strict template. We will define clear sections using custom tags like `[THOUGHTS]` and `[REFACTORED_CODE]` and instruct the model to place its response *only* within these sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e6ba16-7867-4f61-85fc-ced87d8d7b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cc0dd1-a4df-44e2-bcdb-151175271400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
