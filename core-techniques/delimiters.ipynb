{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b25fc04-981a-4d01-a68b-289774c76306",
   "metadata": {},
   "source": [
    "# The Power of Delimiters\n",
    "\n",
    "Welcome! We know that a good prompt is built on the \"Three Pillars\" of Instruction, Context, and Constraints. But how do we organize these pillars within the prompt itself to ensure the model understands where one ends and the next begins?\n",
    "\n",
    "The answer is one of the simplest yet most effective techniques in prompt engineering: **using delimiters.**\n",
    "\n",
    "A delimiter is just a set of characters that marks the boundary of a piece of text. By using them, you transform your prompt from a confusing \"wall of text\" into a structured, well-organized document that is easy for the model to parse. This simple act dramatically increases the reliability of your outputs.\n",
    "\n",
    "In this notebook, we'll see the power of delimiters in action. We'll start with a poorly structured prompt and show how adding different types of delimiters makes it far more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4edd81d-223d-458f-a8c3-9c3a0762bba1",
   "metadata": {},
   "source": [
    "## Setup: Helper Functions and Context\n",
    "\n",
    "First, let's set up our reusable helper functions and define the context for our task. The task will be a common one for developers: **summarizing a technical JSON API response into a human-readable bug report.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11779d6-91c0-4d1a-b3fa-46149f0bddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import litellm\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from textwrap import dedent\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-4o-mini\"\n",
    "MAX_TOKENS_DEFAULT = 200\n",
    "\n",
    "def get_completion(\n",
    "    prompt,\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=MAX_TOKENS_DEFAULT,\n",
    "    **kwargs\n",
    "):\n",
    "    parsed_messages = []\n",
    "\n",
    "    if type(prompt) is str:\n",
    "        parsed_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        parsed_messages = prompt\n",
    "\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=parsed_messages,\n",
    "        max_tokens=max_tokens,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# --- Our Context Data ---\n",
    "\n",
    "bug_ticket_json = {\n",
    "  \"ticket_id\": \"BUG-4521\",\n",
    "  \"status\": \"Open\",\n",
    "  \"priority\": \"High\",\n",
    "  \"component\": \"api-v2\",\n",
    "  \"title\": \"User authentication fails with expired token\",\n",
    "  \"logs\": [\n",
    "    {\"timestamp\": \"2024-07-26T10:00:15Z\", \"level\": \"ERROR\", \"message\": \"Token validation failed: expired\"},\n",
    "    {\"timestamp\": \"2024-07-26T10:00:15Z\", \"level\": \"INFO\", \"message\": \"Attempting re-authentication for user: user_abc\"}\n",
    "  ],\n",
    "  \"reporter\": \"dev_jane\"\n",
    "}\n",
    "\n",
    "# Convert the Python dict to a JSON string for use in the prompt\n",
    "bug_ticket_str = json.dumps(bug_ticket_json, indent=2)\n",
    "\n",
    "print(\"Helper functions and context data are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede9b68a-d416-47f4-9d6c-08742e5ae221",
   "metadata": {},
   "source": [
    "## The \"Before\": A Poorly Structured Prompt\n",
    "\n",
    "Let's start by creating a prompt with no clear structure. We'll blend the instruction, the JSON data, and our constraints into a single paragraph. This is a common mistake that can confuse the model and lead to unreliable results, especially when using cheaper, less powerful models. **This can also confuse us, as we need to be much more careful when inspecting and modifying the prompt!**\n",
    "\n",
    "The model might get this right sometimes, but it's risky. It has to work harder to figure out what part of the prompt is the instruction versus the data to be operated on. This can lead to errors, especially with more complex inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5ee765-a2c9-4433-a588-e3694295875c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8385caf4-2733-43ba-b312-eb3f4a405e2a",
   "metadata": {},
   "source": [
    "## The \"After\": Adding Structure with Delimiters\n",
    "Now, let's refactor the exact same prompt using different delimiter styles. Notice how much clearer and more organized the prompts become. This clarity is valuable for you as a developer and for the model as the processor.\n",
    "\n",
    "### Delimiter Style 1: Markdown Headers\n",
    "Using Markdown-style headers (###) is highly readable and is an excellent way to label the different pillars of your prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434a56fd-eeeb-45c8-869d-11ee07f7dafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6d3c2c0-02c1-4ad8-9bf7-407e5f16d8e5",
   "metadata": {},
   "source": [
    "### Delimiter Style 2: XML Tags\n",
    "\n",
    "Using custom XML-style tags is extremely explicit and unambiguous for the model. It clearly marks the beginning and end of each block of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23150ec1-6f10-4c98-a6eb-32a33c44fea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58e01c5d-277c-4f0d-9a82-2ca8cad2d278",
   "metadata": {},
   "source": [
    "### Delimiter Style 3: Triple Backticks\n",
    "\n",
    "Triple backticks are a developer's favorite. Using them with a language identifier (like `json`) gives the model a strong hint about the format of the context data, which can further improve parsing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee5420-1c83-463d-9bbc-9632b208086c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ba4c0d-ba2d-4eaf-8400-70c1d78ae787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
