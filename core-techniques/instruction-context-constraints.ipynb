{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4317c658-187b-4d4d-9340-2093abb1fee8",
   "metadata": {},
   "source": [
    "# The Three Pillars - Instruction, Context, and Constraints\n",
    "\n",
    "Welcome! This notebook brings our most important prompt design framework to life: **The Three Pillars**. As we discussed, nearly every effective prompt is a combination of these three components:\n",
    "\n",
    "1. **Instruction:** What you want the model to *do*. (e.g., \"Write unit tests.\")\n",
    "2. **Context:** The information the model *needs* to perform the instruction. (e.g., The code to be tested.)\n",
    "3. **Constraints:** The *rules* the output must follow. (e.g., \"Use the `unittest` framework\" or \"Format as JSON.\")\n",
    "\n",
    "To see this in action, we'll perform a common developer task: **generating unit tests for a Python function.** We will build our prompt iteratively, starting with just an instruction and adding pillars one by one to see how the quality of the output dramatically improves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e235633-fbdf-4812-ae22-06aaac60020c",
   "metadata": {},
   "source": [
    "## The Function to be Tested (Our Context)\n",
    "\n",
    "First, let's define the code we want to test. This Python function will serve as the **Context** for our prompt later on. It's designed to find the longest string in a list and includes a few edge cases we'll want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450ae47d-58d7-4937-96c1-d0e2db7ab981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function we will ask the LLM to write tests for.\n",
    "\n",
    "def find_longest_string(strings: list[str]) -> str | None:\n",
    "    \"\"\"\n",
    "    Given a list of strings, finds and returns the longest string.\n",
    "\n",
    "    - If the list is empty, it returns None.\n",
    "    - If there's a tie, it returns the first one found.\n",
    "    - It raises a TypeError if the list contains non-string elements.\n",
    "    \"\"\"\n",
    "    if not strings:\n",
    "        return None\n",
    "\n",
    "    longest_string = \"\"\n",
    "    for s in strings:\n",
    "        if not isinstance(s, str):\n",
    "            raise TypeError(\"All elements in the list must be strings.\")\n",
    "        if len(s) > len(longest_string):\n",
    "            longest_string = s\n",
    "\n",
    "    return longest_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1007e0-cb7c-4ea1-9b19-48fb0a4eac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "function_source_code = inspect.getsource(find_longest_string)\n",
    "\n",
    "print(\"--- Function to be tested ---\")\n",
    "print(function_source_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682d9af-e9d9-4927-87cf-1a32c4ba181f",
   "metadata": {},
   "source": [
    "## Setup: Reusable Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f9770-fb8d-40b2-a41d-445292d3dd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import litellm\n",
    "from dotenv import load_dotenv\n",
    "from textwrap import dedent\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-4o-mini\"\n",
    "MAX_TOKENS_DEFAULT = 200\n",
    "\n",
    "def get_completion(\n",
    "    prompt,\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=MAX_TOKENS_DEFAULT,\n",
    "    **kwargs\n",
    "):\n",
    "    parsed_messages = []\n",
    "\n",
    "    if type(prompt) is str:\n",
    "        parsed_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        parsed_messages = prompt\n",
    "\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=parsed_messages,\n",
    "        max_tokens=max_tokens,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"Helper functions are defined and ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625d5d95-22ab-4368-8b10-9db5e70d9e5d",
   "metadata": {},
   "source": [
    "## Attempt 1: Using Only an Instruction (Pillar 1)\n",
    "\n",
    "Let's start with the most basic prompt possible: just an instruction. We'll ask the model to perform a task without giving it any of the necessary information.\n",
    "\n",
    "As you'll see, the model can't read our minds. Without context, the instruction is ambiguous and the model has to guess, often resulting in a generic, unhelpful, or incomplete response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657c460-95f5-4341-9d46-ac8ea5d4d457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a6d2eb2-db51-48d5-a39c-66cb1b550401",
   "metadata": {},
   "source": [
    "## Attempt 2: Adding Context (Pillars 1 & 2)\n",
    "\n",
    "Now, let's provide the second pillar: **Context**. We'll give the model the same instruction, but this time we'll also provide the source code of the function we want it to test.\n",
    "\n",
    "This is a huge improvement. The model now has the necessary information to perform the task. However, as we'll see, without constraints, the output might not be exactly what we need for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb309128-464a-4e89-889e-d58c774e483a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17d3588a-2656-400c-aaec-9f41c3040edf",
   "metadata": {},
   "source": [
    "## Attempt 3: Adding Constraints for Precision (Pillars 1, 2, & 3)\n",
    "\n",
    "This is the final and most powerful step. We will add the third pillar: **Constraints**. We will provide the same instruction and context, but now we'll add a specific set of rules that the output must follow. This gives us precise control over the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a420d1-fb0a-4355-bc1e-78d12bc31259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
