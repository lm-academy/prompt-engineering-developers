{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2747107-0408-4622-9a9d-68fde0996fa7",
   "metadata": {},
   "source": [
    "# Function Calling & Tool Use - Giving LLMs Superpowers\n",
    "\n",
    "Hello everyone. Today, we explore a feature that elevates an LLM from a simple text generator into an active agent that can interact with the outside world: **Function Calling**, also known as **Tool Use**.\n",
    "\n",
    "Until now, the LLM has been in a closed box. It can't browse the web, access your database, or call an API. Function calling is the mechanism that breaks open that box. It allows the model to ask *our application* to run a function and return the result, giving it access to live data and real-world capabilities.\n",
    "\n",
    "This notebook will walk you through a complete, end-to-end example of building a simple weather bot that uses a \"tool\" (a local Python function) to get real-time information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6ec293-c2d0-4e0d-8057-37b2c6043c40",
   "metadata": {},
   "source": [
    "## Defining Our \"Tool\"\n",
    "\n",
    "First, let's define the tool we want to give to the LLM. For this demonstration, we'll create a mock weather function. In a real application, this function would call a live weather API.\n",
    "\n",
    "Our tool is a simple Python function called `get_current_weather`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ed8b6-9a8c-4254-930b-e49f77576944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "from textwrap import dedent\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-4o-mini\"\n",
    "MAX_TOKENS_DEFAULT = 500\n",
    "\n",
    "def get_completion(\n",
    "    prompt,\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=MAX_TOKENS_DEFAULT,\n",
    "    **kwargs\n",
    "):\n",
    "    if \"gpt-5\" in model:\n",
    "        kwargs[\"max_completion_tokens\"] = max_tokens\n",
    "    else:\n",
    "        kwargs[\"max_tokens\"] = max_tokens\n",
    "        \n",
    "    parsed_messages = []\n",
    "\n",
    "    if type(prompt) is str:\n",
    "        parsed_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        parsed_messages = prompt\n",
    "\n",
    "    return litellm.completion(\n",
    "        model=model,\n",
    "        messages=parsed_messages,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "# --- Our Local Tool (Python Function) ---\n",
    "def get_current_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Gets the current weather for a given location.\n",
    "\n",
    "    Args:\n",
    "        location (str): The city and state, e.g., \"San Francisco, CA\".\n",
    "\n",
    "    Returns:\n",
    "        A JSON string with the weather information.\n",
    "    \"\"\"\n",
    "    print(f\"--- TOOL: Calling get_current_weather(location='{location}') ---\")\n",
    "    # In a real app, this would call a live weather API.\n",
    "    # We'll use mock data for this example.\n",
    "    if \"boston\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Boston\", \"temperature\": \"18°C\", \"forecast\": \"cloudy\"})\n",
    "    elif \"berlin\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Berlin\", \"temperature\": \"15°C\", \"forecast\": \"sunny\"})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n",
    "\n",
    "print(\"Setup complete. Helper functions and context are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb9d9b-3469-468c-a84c-7eb91e050b32",
   "metadata": {},
   "source": [
    "## Describing the Tool to the Model\n",
    "\n",
    "We don't send the model our Python code. Instead, we send a **description** of the tool. This description follows a specific JSON schema, defining the function's name, its purpose, and the parameters it accepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e1a459-3f8e-42d4-b393-083e1d2d1756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42ecb69c-4224-4061-a938-291db1cfbbf9",
   "metadata": {},
   "source": [
    "## The First API Call - Model Decides to Use the Tool\n",
    "\n",
    "Now, we make our first API call. We pass the user's question and the `tools` schema. The model will see the question, realize it can't answer from its internal knowledge, and see that our `get_current_weather` tool is a perfect fit.\n",
    "\n",
    "Instead of returning a text message, it will return a special `tool_calls` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9ecdb4-9188-4ae7-89b6-fa1c841ab80c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "427700dd-81f9-4e30-b6db-dee17a5c5b6b",
   "metadata": {},
   "source": [
    "## Executing the Tool and Sending the Result Back\n",
    "\n",
    "Our code now needs to handle this \"tool call\" request. We will:\n",
    "\n",
    "1. Parse the `tool_calls` object to get the function name and arguments.\n",
    "2. Call our local Python function with those arguments.\n",
    "3. Append the result of our function call to the message history with a new `tool` role.\n",
    "4. Make a second API call with this updated history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a706d14-9abc-467a-b08c-33852462dbda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b0db8-518b-4aea-a095-fbf6629f1c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
