{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ff09b5-5722-47d1-96e5-87da072fde34",
   "metadata": {},
   "source": [
    "# The Decomposition Pattern - Breaking Down Big Problems\n",
    "\n",
    "Hello everyone, let's now explore the **Decomposition Pattern**, which can be very useful in tackling more complex problems.\n",
    "\n",
    "The core idea is simple but powerful: **Don't solve one big problem. Solve many small problems.**\n",
    "\n",
    "We break down a complex task into a sequence of smaller, focused sub-tasks. Each sub-task is handled by its own dedicated prompt, creating a \"pipeline\" where the output of one step becomes the input for the next.\n",
    "\n",
    "This notebook will demonstrate this by building a three-step pipeline for a common developer workflow:\n",
    "\n",
    "1. **Step 1:** Identify bugs in a piece of code.\n",
    "2. **Step 2:** Refactor the code to fix the identified bugs.\n",
    "3. **Step 3:** Generate unit tests for the newly refactored code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c353c8-db68-449a-9f41-dd0a24fac5f8",
   "metadata": {},
   "source": [
    "## The Task and Helper Functions\n",
    "\n",
    "First, let's define our helper functions and the initial \"buggy\" code that we will process through our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a656ec9-3173-490f-9f8a-f702ebb11bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "import inspect\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "from textwrap import dedent\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-4o-mini\"\n",
    "MAX_TOKENS_DEFAULT = 500\n",
    "\n",
    "def get_completion(\n",
    "    prompt,\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=MAX_TOKENS_DEFAULT,\n",
    "    **kwargs\n",
    "):\n",
    "    if \"gpt-5\" in model:\n",
    "        kwargs[\"max_completion_tokens\"] = max_tokens\n",
    "    else:\n",
    "        kwargs[\"max_tokens\"] = max_tokens\n",
    "        \n",
    "    parsed_messages = []\n",
    "\n",
    "    if type(prompt) is str:\n",
    "        parsed_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        parsed_messages = prompt\n",
    "\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=parsed_messages,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_average_score(student_records):\n",
    "    total_score = 0\n",
    "    for record in student_records:\n",
    "        total_score += record['score']\n",
    "\n",
    "    return total_score / len(student_records)\n",
    "\n",
    "CODE_TO_PROCESS = inspect.getsource(get_average_score)\n",
    "\n",
    "print(\"Setup complete. Helper functions and code context are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395e4c43-0ca9-47ef-8e13-970d70d21227",
   "metadata": {},
   "source": [
    "## Bug Identification\n",
    "\n",
    "Our first prompt is highly focused. Its only job is to analyze the code and identify potential bugs. We'll ask for the output in JSON format so we can easily parse it for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45faf71-3494-4c44-bd46-c9d8e0212024",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_bug_identification = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": dedent(\"\"\"\n",
    "        You are a master code auditor.\n",
    "        Your sole task is to analyze Python code and identify potential bugs, errors, and edge-cases.\n",
    "\n",
    "        Do NOT suggest fixes.\n",
    "\n",
    "        Output your findings as a JSON object with the following schema:\n",
    "        \n",
    "        {\n",
    "            \"bugs\": list(str)\n",
    "        }\n",
    "        \"\"\")\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": dedent(f\"\"\"\n",
    "        Please analyze the following Python code:\n",
    "\n",
    "        ```python\n",
    "        {CODE_TO_PROCESS}\n",
    "        ```\n",
    "        \"\"\")\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"--- Step 1: Identifying bugs ---\")\n",
    "identified_bugs_json = get_completion(\n",
    "    prompt_bug_identification,\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "if identified_bugs_json:\n",
    "    print(\"Identified issues:\")\n",
    "    print(identified_bugs_json)\n",
    "    identified_bugs = json.loads(identified_bugs_json).get(\"bugs\", [])\n",
    "else:\n",
    "    identified_bugs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f5ad9-302a-4a8a-abaf-0d2515bb95e0",
   "metadata": {},
   "source": [
    "## Refactoring\n",
    "\n",
    "Now, we create our second prompt. Its context includes **both** the original code and the list of bugs we identified in Step 1. The task is to fix these specific issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34319eb0-d48b-4083-ab75-419d006ac78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_refactoring = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": dedent(\"\"\"\n",
    "        You are a senior Python developer.\n",
    "        Your task is to refactor code to fix identified issues and improve its quality and robustness.\n",
    "        \"\"\")\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": dedent(f\"\"\"\n",
    "        Please refactor the following Python code to fix the idenfied issues.\n",
    "        Add a docstring and type hints.\n",
    "\n",
    "        ## Original code\n",
    "        ```python\n",
    "        {CODE_TO_PROCESS}\n",
    "        ```\n",
    "\n",
    "        ## Identified issues\n",
    "        {\"* \".join(identified_bugs)}\n",
    "\n",
    "        ## Response instructions\n",
    "        * Your response ahould be only the complete, refactored Python code.\n",
    "        * Do not output any triple backticks nor code blocks, just directly the code.\n",
    "        * Output only valid Python code.\n",
    "        \"\"\")\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"--- Step 2: Refactoring the code ---\")\n",
    "refactored_code = get_completion(prompt_refactoring)\n",
    "\n",
    "if refactored_code:\n",
    "    print(\"Refactored code:\")\n",
    "    print(refactored_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb605379-ce38-4e65-a02c-bb073ef7b86e",
   "metadata": {},
   "source": [
    "## Test Generation\n",
    "\n",
    "Finally, our third prompt takes the **refactored code from Step 2** as its context. Its single, focused goal is to generate a comprehensive suite of unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b983b5-ad7d-40a6-bdfe-af5e06362b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_generate_unit_tests = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": dedent(\"\"\"\n",
    "        You are an expert QA engineer specializing in Python.\n",
    "        Your task is to write a comprehensive suite of unit tests for a given function.\n",
    "        You always use the pytest framework.\n",
    "        \"\"\")\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": dedent(f\"\"\"\n",
    "        Please write unit tests for the following refactored Python function.\n",
    "\n",
    "        The tests should cover the main functionality and edge-cases.\n",
    "        \n",
    "        Output the content in XML strictly following the structure in the Output section.\n",
    "        Do not output any additional content outside of the XML tags.\n",
    "\n",
    "        ## Refactored code\n",
    "        {refactored_code}\n",
    "\n",
    "        ## Output\n",
    "        <preamble>\n",
    "            Add your preamble here.\n",
    "        </preamble>\n",
    "        <reasoning>\n",
    "            Add your reasoning here.\n",
    "        </reasoning>\n",
    "        <edge_cases>\n",
    "            Add all identified edge-cases here.\n",
    "        </edge_cases>\n",
    "        <python_code>\n",
    "            Add the test code here.\n",
    "        </python_code>\n",
    "        <conclusion>\n",
    "            Add the conclusion here\n",
    "        </conclusion>\n",
    "        \"\"\")\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"--- Step 3: Generating unit tests ---\")\n",
    "unit_tests = get_completion(prompt_generate_unit_tests, max_tokens=2000)\n",
    "\n",
    "if unit_tests:\n",
    "    print(\"Generated tests:\")\n",
    "    print(unit_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7607824e-f69d-4c8c-bd0d-023087419bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
