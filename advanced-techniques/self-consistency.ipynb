{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "868d0780-66b7-4712-9ac8-15fa65e4eab3",
   "metadata": {},
   "source": [
    "# The Self-Consistency Pattern - A \"Generate-and-Synthesize\" Approach\n",
    "\n",
    "Hello everyone. We've seen how Chain-of-Thought can produce a single, high-quality reasoning path. Today, we're exploring a more advanced application of the **Self-Consistency** pattern, moving beyond a simple \"majority vote\" to a powerful \"generate-and-synthesize\" workflow.\n",
    "\n",
    "The core idea is a two-step process:\n",
    "\n",
    "1. **Diverge (Generate):** First, we ask the model to generate multiple, diverse solutions to a creative problem.\n",
    "2. **Converge (Synthesize):** Then, we feed all those solutions into a second, \"editor\" prompt that synthesizes them into a single, superior final output.\n",
    "\n",
    "This mimics a real-world creative team: a brainstorming session followed by an editing session. We'll demonstrate this by generating high-quality documentation for a Python function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1841c60-8c73-4266-ba20-24a960ab6b03",
   "metadata": {},
   "source": [
    "## The Task and Helper Functions\n",
    "\n",
    "Our task is to generate a comprehensive docstring for a Python function that is complex enough to benefit from detailed documentation. It takes multiple arguments and has a clear return value and potential error cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d3be82-4bff-44d4-9d9d-17765ef9c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "import inspect\n",
    "from IPython.display import display, Markdown\n",
    "from textwrap import dedent\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-4o-mini\"\n",
    "MAX_TOKENS_DEFAULT = 500\n",
    "\n",
    "def get_completion(\n",
    "    prompt,\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=MAX_TOKENS_DEFAULT,\n",
    "    **kwargs\n",
    "):\n",
    "    if \"gpt-5\" in model:\n",
    "        kwargs[\"max_completion_tokens\"] = max_tokens\n",
    "    else:\n",
    "        kwargs[\"max_tokens\"] = max_tokens\n",
    "        \n",
    "    parsed_messages = []\n",
    "\n",
    "    if type(prompt) is str:\n",
    "        parsed_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        parsed_messages = prompt\n",
    "\n",
    "    return litellm.completion(\n",
    "        model=model,\n",
    "        messages=parsed_messages,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "# --- The Context: A More Complex Function to Document ---\n",
    "def parse_log_entry(entry_line: str, strict_mode: bool = True) -> dict | None:\n",
    "    \"\"\"\n",
    "    Example format: \"ERROR|2024-07-27T10:00:00Z|User login failed\"\n",
    "    \"\"\"\n",
    "    parts = entry_line.split('|', 2)\n",
    "    if len(parts) != 3:\n",
    "        if strict_mode:\n",
    "            raise ValueError(\"Log entry must have 3 parts separated by vertical bars.\")\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"level\": parts[0],\n",
    "        \"timestamp\": parts[1],\n",
    "        \"message\": parts[2]\n",
    "    }\n",
    "\n",
    "CODE_TO_DOCUMENT = inspect.getsource(parse_log_entry)\n",
    "\n",
    "print(\"Setup complete. Function context is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4533ba4d-0b86-4bc9-b1e3-d5ed1e6a130e",
   "metadata": {},
   "source": [
    "## The Generation Phase (Divergence)\n",
    "\n",
    "First, we will ask the model to generate three different docstring drafts. We'll use a single prompt but set `n=3` and a non-zero `temperature` to encourage slightly different wording, focus, and examples in each draft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d511581-0d83-4889-b0a7-b043b1ae7f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "857a4d45-3ec8-46dd-ae0d-6f17e3c29d99",
   "metadata": {},
   "source": [
    "## The Synthesis Phase (Convergence)\n",
    "\n",
    "Now for the powerful part. We will feed all three drafts into a new prompt. This prompt uses an \"Editor\" persona whose only job is to combine the best elements of each draft into a single, polished, final version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a84a50-3683-4468-89ad-e3b237f22e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6760fcf9-c15f-4eb7-bfbf-370fa74d37fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
