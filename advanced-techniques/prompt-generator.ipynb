{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f2dc644-5004-4c39-8f5a-e99ab8374b30",
   "metadata": {},
   "source": [
    "# The Prompt Generator Pattern - AI as a Prompt Engineer\n",
    "\n",
    "Hello everyone. Today, we're exploring a fascinating \"meta\" technique where we use an AI to help us do our job as prompt engineers. So far, we have been carefully crafting prompts. But what if we could use the AI itself to build better prompts for us?\n",
    "\n",
    "This is the **Prompt Generator Pattern**. The core idea is to use a powerful, state-of-the-art model to act as your expert prompt engineering partner. Its task is not to solve the final problem, but to **generate a high-quality, optimized prompt** that you can then use for your actual task.\n",
    "\n",
    "This is especially useful when you want to use a cheaper, faster model for a repetitive production task but need the prompt to be perfectly optimized for reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5140986c-5e0b-45ea-b3d8-cd588aae8add",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "First, let's set up our standard helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c2c006-474f-4848-bc89-4de326788c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "from IPython.display import display, Markdown\n",
    "from textwrap import dedent\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-4o-mini\"\n",
    "MAX_TOKENS_DEFAULT = 500\n",
    "\n",
    "def get_completion(\n",
    "    prompt,\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=MAX_TOKENS_DEFAULT,\n",
    "    **kwargs\n",
    "):\n",
    "    if \"gpt-5\" in model:\n",
    "        kwargs[\"max_completion_tokens\"] = max_tokens\n",
    "    else:\n",
    "        kwargs[\"max_tokens\"] = max_tokens\n",
    "        \n",
    "    parsed_messages = []\n",
    "\n",
    "    if type(prompt) is str:\n",
    "        parsed_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        parsed_messages = prompt\n",
    "\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=parsed_messages,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"Setup complete. Helper functions and code context are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdf193-7dfc-48c6-9add-7fdfdd51f154",
   "metadata": {},
   "source": [
    "## The Master Prompt (Generating a Prompt)\n",
    "\n",
    "Our first step is to craft a \"master prompt.\" This prompt doesn't solve the final task; it *describes the prompt we want*. We will go ahead and implement a conversation flow to interactively craft our prompt. (You can also just do this exercise in ChatGPT, Claude Desktop, or any other AI application)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c14e94-dec4-4000-8b35-ebfb16150d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": dedent(\"\"\"\n",
    "        You are an expert prompt engineer, and your task is to create a high-quality,\n",
    "        optimized system prompt based on a user's specification.\n",
    "\n",
    "        ## Process:\n",
    "\n",
    "        1. Start by asking me to provide the task I want you to generate a prompt for.\n",
    "        2. Once I provide the task, ask me questions to clarify any doubts or missing information.\n",
    "        3. Once you have the necessary information, create a detailed and effective system prompt that\n",
    "            I can use with an AI system to tackle the task at hand.\n",
    "        4. When I write \"GENERATE\", you will generate the final prompt based on the information\n",
    "            we have discussed.\n",
    "\n",
    "        ## Generated prompt output rules:\n",
    "        * Output only the text of the generated prompt between <prompt>[GENERATED PROMPT]</prompt>\n",
    "        * Do not output any triple backticks nor code blocks, just the contents of the generated prompt \n",
    "            between the XML tags.\n",
    "        * Leverage markdown and prompt engineering best practices to structure the prompt.\n",
    "\n",
    "        ## Examples of responses for generated prompts:\n",
    "        <prompt>\n",
    "        You are a knowledgeable and supportive AI DevOps tutor specializing in automation, observability, and AWS. Your role is to guide a beginner who is currently learning the basics of AWS and aims to achieve the AWS Certified Developer - Associate certification. \n",
    "\n",
    "        Provide practical, hands-on learning experiences and real-world project ideas that align with my learning goals. Break down complex topics into understandable segments and include the following: \n",
    "        \n",
    "        1. **Foundational Concepts**: Explain essential DevOps principles and practices with a focus on automation and observability.\n",
    "        2. **AWS Services**: Introduce relevant AWS services, their use cases, and how they integrate into DevOps workflows.\n",
    "        3. **Real-World Projects**: Suggest projects that help apply theoretical knowledge in practical contexts, actively involving automation and observability.\n",
    "        4. **Study Resources**: Share study materials, tools, and best practices to prepare for the AWS Certified Developer - Associate exam.\n",
    "        5. **Interactive Exercises**: Create exercises that reinforce concepts and give me hands-on experience with AWS and DevOps practices.\n",
    "        \n",
    "        Encourage questions and provide answers constructively, promoting a clear understanding and a smooth learning journey towards certification.\n",
    "        </prompt>\n",
    "        \"\"\")\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e20cd-f617-4395-8e46-2402be1bc7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = master_prompt.copy()\n",
    "generated_prompt = None\n",
    "\n",
    "print(\"Starting interactive prompt generation session...\")\n",
    "print(\"Type 'quit' to exit the conversation.\\n\")\n",
    "\n",
    "initial_response = get_completion(conversation_history)\n",
    "print(\"AI: \", initial_response)\n",
    "print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \").strip()\n",
    "\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"stop\"]:\n",
    "        print(\"Ending conversation.\")\n",
    "        break\n",
    "\n",
    "    conversation_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_input\n",
    "    })\n",
    "\n",
    "    ai_response = get_completion(conversation_history)\n",
    "\n",
    "    conversation_history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": ai_response\n",
    "    })\n",
    "\n",
    "    print(\"AI: \", ai_response)\n",
    "    print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "    if \"<prompt>\" in ai_response:\n",
    "        generated_prompt = ai_response.split(\"<prompt>\")[1].rsplit(\"</prompt>\")[0].strip()\n",
    "        print(\"Final prompt has been generated and stored!\")\n",
    "        break\n",
    "\n",
    "if generated_prompt:\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    print(\"Generated prompt:\\n\")\n",
    "    print(generated_prompt)\n",
    "else:\n",
    "    print(\"No final prompt was generated in this session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba5350-92f2-4c9e-b54f-b96f3b367b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
