{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f2dc644-5004-4c39-8f5a-e99ab8374b30",
   "metadata": {},
   "source": [
    "# The Prompt Generator Pattern - AI as a Prompt Engineer\n",
    "\n",
    "Hello everyone. Today, we're exploring a fascinating \"meta\" technique where we use an AI to help us do our job as prompt engineers. So far, we have been carefully crafting prompts. But what if we could use the AI itself to build better prompts for us?\n",
    "\n",
    "This is the **Prompt Generator Pattern**. The core idea is to use a powerful, state-of-the-art model to act as your expert prompt engineering partner. Its task is not to solve the final problem, but to **generate a high-quality, optimized prompt** that you can then use for your actual task.\n",
    "\n",
    "This is especially useful when you want to use a cheaper, faster model for a repetitive production task but need the prompt to be perfectly optimized for reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5140986c-5e0b-45ea-b3d8-cd588aae8add",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "First, let's set up our standard helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c2c006-474f-4848-bc89-4de326788c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "from IPython.display import display, Markdown\n",
    "from textwrap import dedent\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-4o-mini\"\n",
    "MAX_TOKENS_DEFAULT = 500\n",
    "\n",
    "def get_completion(\n",
    "    prompt,\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=MAX_TOKENS_DEFAULT,\n",
    "    **kwargs\n",
    "):\n",
    "    if \"gpt-5\" in model:\n",
    "        kwargs[\"max_completion_tokens\"] = max_tokens\n",
    "    else:\n",
    "        kwargs[\"max_tokens\"] = max_tokens\n",
    "        \n",
    "    parsed_messages = []\n",
    "\n",
    "    if type(prompt) is str:\n",
    "        parsed_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        parsed_messages = prompt\n",
    "\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=parsed_messages,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"Setup complete. Helper functions and code context are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdf193-7dfc-48c6-9add-7fdfdd51f154",
   "metadata": {},
   "source": [
    "## The Master Prompt (Generating a Prompt)\n",
    "\n",
    "Our first step is to craft a \"master prompt.\" This prompt doesn't solve the final task; it *describes the prompt we want*. We will go ahead and implement a conversation flow to interactively craft our prompt. (You can also just do this exercise in ChatGPT, Claude Desktop, or any other AI application)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c14e94-dec4-4000-8b35-ebfb16150d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
