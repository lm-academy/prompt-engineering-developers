{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0c9e76-8e7c-47b1-a214-a038157db4fd",
   "metadata": {},
   "source": [
    "# The Flip the Script Pattern - Forcing Clarification and Exploration\n",
    "\n",
    "Hello everyone. In nearly every pattern so far, the dynamic has been the same: we give a task, and the AI provides an answer. But what if the task is ambiguous? What if we want the model to make fewer assumptions about the task it is about to complete?\n",
    "\n",
    "The **\"Flip the Script\" Pattern** changes this dynamic. Instead of getting an answer, our goal is to get **intelligent questions or a set of well-defined options** from the AI. This turns the model from a passive answering machine into a proactive, Socratic partner, helping us refine our thinking before committing to a solution.\n",
    "\n",
    "This notebook will demonstrate the two primary use cases for this pattern:\n",
    "\n",
    "1. Forcing the AI to ask clarifying questions for an ambiguous task.\n",
    "2. Asking the AI to explore a solution space and present options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd3b349-27c1-4911-96c7-19ef522f640c",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "First, let's set up our standard helper functions. The \"Flip the Script\" pattern is entirely about how we craft the prompt, so our standard functions will work perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad14d8f-b289-43e4-a640-db5a10850685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "from IPython.display import display, Markdown\n",
    "from textwrap import dedent\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-4o-mini\"\n",
    "MAX_TOKENS_DEFAULT = 500\n",
    "\n",
    "def get_completion(\n",
    "    prompt,\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=MAX_TOKENS_DEFAULT,\n",
    "    **kwargs\n",
    "):\n",
    "    if \"gpt-5\" in model:\n",
    "        kwargs[\"max_completion_tokens\"] = max_tokens\n",
    "    else:\n",
    "        kwargs[\"max_tokens\"] = max_tokens\n",
    "        \n",
    "    parsed_messages = []\n",
    "\n",
    "    if type(prompt) is str:\n",
    "        parsed_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        parsed_messages = prompt\n",
    "\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=parsed_messages,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"Setup complete. Helper functions and code context are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da2aeb-138c-402e-9f3d-8d03aadaf148",
   "metadata": {},
   "source": [
    "## Use Case 1: Forcing Clarification on Ambiguous Tasks\n",
    "\n",
    "As developers, we often start with a vague requirement like \"build an auth system.\" A bad prompt would ask the AI to just generate the code, forcing it to make dozens of incorrect assumptions.\n",
    "\n",
    "A good prompt \"flips the script\" and uses the AI's knowledge to help us define the problem better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9423396a-9cbf-4e07-87d7-798fb5612736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65de0adc-bc48-45f4-96f9-2f7aa6f20684",
   "metadata": {},
   "source": [
    "## Use Case 2: Exploring an Open-Ended Solution Space\n",
    "\n",
    "Sometimes the problem isn't ambiguous, but the number of possible solutions is overwhelming. We can \"flip the script\" to turn the AI into a consultant that lays out our options and lets us choose the path forward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482253a0-bb87-42ae-9fbd-5ccb27c3e9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
